<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models - Jing Lyu">
  <meta name="description" content="LaRA-VLA internalizes chain-of-thought reasoning into continuous latent representations for efficient vision-language-action control, reducing inference latency while improving performance on simulation and real-robot tasks.">
  <meta name="keywords" content="vision-language-action, VLA, chain-of-thought, latent reasoning, embodied AI, robotics, manipulation, imitation learning">
  <meta name="author" content="Jing Lyu">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Latent-Reasoning-VLA">
  <meta property="og:title" content="Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models">
  <meta property="og:description" content="LaRA-VLA internalizes chain-of-thought reasoning into continuous latent representations for efficient vision-language-action control, reducing inference latency while improving performance on simulation and real-robot tasks.">
  <!-- NOTE: Using relative paths for local preview/static hosting. For best social sharing, use absolute URLs. -->
  <meta property="og:url" content="./">
  <meta property="og:image" content="static/images/2.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Latent Reasoning VLA - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Jing Lyu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="vision-language-action">
  <meta property="article:tag" content="latent reasoning">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Latent Reasoning VLA (LaRA-VLA)">
  <meta name="twitter:description" content="Latent chain-of-thought reasoning for efficient vision-language-action control. Up to 90% lower inference latency vs explicit CoT while improving performance on simulation and real-robot tasks.">
  <meta name="twitter:image" content="static/images/2.png">
  <meta name="twitter:image:alt" content="Latent Reasoning VLA - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models">
  <meta name="citation_author" content="Lyu, Jing">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="ICML 2026">
  <meta name="citation_pdf_url" content="static/pdfs/ICML_2026_Latent_Reasoning_VLA.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Latent Reasoning VLA (LaRA-VLA) | Project Page</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/vlas.ico">
  <link rel="apple-touch-icon" href="static/images/vlas.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models",
    "description": "LaRA-VLA internalizes chain-of-thought reasoning into continuous latent representations for efficient vision-language-action control.",
    "author": [
      {
        "@type": "Person",
        "name": "Jing Lyu",
        "affiliation": {
          "@type": "Organization",
          "name": "Institution Name"
        }
      }
    ],
    "datePublished": "2026-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "Under review"
    },
    "url": "./",
    "image": "static/images/2.png",
    "keywords": ["vision-language-action", "VLA", "chain-of-thought", "latent reasoning", "embodied AI", "robotics"],
    "abstract": "Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (LaRA-VLA), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control.",
    "citation": "@misc{lyu2026lara_vla,\\n  title={Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models},\\n  author={Lyu, Jing},\\n  year={2026},\\n  note={Under review}\\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "./"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Vision-Language-Action"
      },
      {
        "@type": "Thing",
        "name": "Embodied AI"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Institution Name",
    "url": "./",
    "logo": "static/images/vlas.ico"
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown (restore this block if you want to showcase more works) -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Shuanghao Bai<sup>* 1,2</sup></span>
              <span class="author-block">Jing Lyu<sup>* 2,3,4</sup></span>
              <span class="author-block">Wanqi Zhou<sup>1</sup></span>
              <span class="author-block">Zhe Li<sup>2</sup></span>
              <span class="author-block">Dakai Wang<sup>1</sup></span>
              <span class="author-block">Lei Xing<sup>1</sup></span>
              <span class="author-block">Xiaoguang Zhao<sup>3</sup></span>
              <span class="author-block">Pengwei Wang<sup>2</sup></span>
              <span class="author-block">Zhongyuan Wang<sup>2</sup></span>
              <span class="author-block">Cheng Chi<sup>2</sup></span>
              <span class="author-block">Badong Chen<sup>1</sup></span>
              <span class="author-block">Shanghang Zhang<sup>2,5</sup></span>
            </div>

            <div class="publication-meta">
              <p class="eql-cntrb">* Equal contribution</p>
              <p class="publication-affiliations">
                <sup>1</sup> Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University.
                <sup>2</sup> Beijing Academy of Artificial Intelligence.
                <sup>3</sup> Institute of Automation, University of Chinese Academy of Sciences.
                <sup>4</sup> School of Artificial Intelligence, University of Chinese Academy of Sciences.
                <sup>5</sup> Peking University.
              </p>
              <p class="publication-affiliations">
                Correspondence to: Cheng Chi, Badong Chen, Shanghang Zhang.
              </p>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/ICML_2026_Latent_Reasoning_VLA.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/LoveJu1y/LaRA-VLA" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener noreferrer">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>
          </div>
        </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            VisionLanguageAction (VLA) models benefit from chainofthought (CoT) reasoning, but existing approaches incur high inference overhead
            and rely on discrete reasoning representations that mismatch continuous perception and control. We propose <b>Latent Reasoning VLA (LaRA-VLA)</b>, a
            unified VLA framework that internalizes multimodal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA
            performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, actionoriented control.
            To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision
            to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both
            simulation benchmarks and long horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms stateoftheart VLA methods while
            reducing inference latency by up to 90% compared to explicit CoTbased approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time
            embodied control.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Abstract -->

<!-- 1. Model Architecture -->
  <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered"> LaRA-VLA</h2>
    <div class="columns is-variable is-6 is-multiline image-pair">
      <div class="column is-12">
        <figure class="figure-card">
          <img src="static/images/2_1.png" alt="Model architecture" loading="lazy">
        </figure>
      </div>
    </div>
    <div class="content has-text-justified">
      <p>
        Training proceeds in three stages: (i) <b>explicit CoT finetuning</b> with aligned visual prediction latents
        and inversedynamics supervision for actions; (ii) a <b>curriculumbased transition</b> from explicit CoT to compact text latents, gradually
        reducing the number of text tokens while increasing reliance on latent reasoning, where the latent representations are also implicitly
        supervised by visual and action signals; and (iii) adaptation of <b>latent-conditioned VLM features</b> to an action expert for efficient action
        generation without explicit CoT at inference time.
      </p>
    </div>
  </div>
</section>
<!-- End Model Architecture -->

<div class="section-divider"></div>

<!-- 2. Experiments -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experiments</h2>

    <h3 class="title is-4">Simulation Experiments</h3>
    <h4 class="title is-5">Libero</h4>
    <div class="figure-card">
      <img src="static/images/3.png" alt="LIBERO results" loading="lazy">
      <p class="figure-caption">
        On <b>LIBERO</b>, LaRA-VLA achieves the best overall performance with an average success rate of <b>97.9%</b>, including 99.8% on
        the Object suite and 96.6% on the Long suite, demonstrating strong object-centric reasoning and robustness in <b>long-horizon</b> manipulation.
      </p>
    </div>

    <h4 class="title is-5">SimplerEnv</h4>
    <div class="figure-card">
      <img src="static/images/5.png" alt="SimplerEnv WidowX results" loading="lazy">
      <p class="figure-caption">
        On <b>SimplerEnv-WidowX</b>, LaRA-VLA attains the highest average success
        rate of 68.8%, outperforming <b>NoCoT</b>, <b>Textual CoT</b>, and <b>Visual CoT</b> baselines. Across both benchmarks, LaRA-VLA consistently surpasses textual and
        visual CoT methods, indicating that latent reasoning provides more effective and stable guidance for action prediction and generalizes better
        than explicit CoT supervision.
      </p>
    </div>

    <h3 class="title is-4">Real-world Experiments</h3>
    <div class="columns is-variable is-6 is-multiline image-pair">
      <div class="column is-12">
        <figure class="figure-card">
          <img src="static/images/6.png" alt="Real-world task results" loading="lazy">
        </figure>
      </div>
    </div>
    <p class="figure-caption">
      <b>LaRA-VLA</b> consistently outperforms <b>ACT</b> and <b>GR00T N1.5</b> across all four <b>long-horizon</b> real-world manipulation tasks, achieving the highest
      average success rate. The improvements are especially pronounced on tasks requiring multi-stage reasoning and sustained temporal coordination,
      highlighting enhanced robustness to error accumulation over long horizons.
    </p>
    <div class="columns is-variable is-4 realworld-compare">
      <div class="column is-6">
        <h4 class="comparison-title">LaRA-VLA</h4>
        <div class="columns is-variable is-2 is-multiline video-grid">
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Pick_Bowls.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Find&Pick_Cube.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Pick_Fruits.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Pick_Objects.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
        </div>
      </div>
      <div class="column is-6">
        <h4 class="comparison-title">Groot N1.5</h4>
        <div class="columns is-variable is-2 is-multiline video-grid">
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Groot_Bowls.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Groot_Cube.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Groot_Fruits.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
          <div class="column is-6">
            <figure class="video-card">
              <video class="realworld-video" controls preload="metadata" playsinline>
                <source src="static/videos/Groot_Objects.mp4" type="video/mp4">
              </video>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Experiments -->

<div class="section-divider"></div>

<!-- 3. Analysis -->
<section class="section analysis-section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Analysis</h2>

    <h3 class="title is-4">Latent Collapse</h3>
    <div class="columns is-variable is-6 is-multiline image-pair">
      <div class="column is-12">
        <figure class="figure-card">
          <img src="static/images/11.png" alt="Latent collapse analysis" loading="lazy">
          <p class="figure-caption">
            Latent tokens associated with different reasoning components form wellseparated and semantically coherent <b>clusters</b>, demonstrating
            clear <b>functional specialization</b> rather than degeneration into uniform or uninformative representations. Moreover, latent
            representations of language instruction tokens (gray points) remain structured and occupy a distinct subspace from reasoning latents, indicating
            that latent CoT does not trivially reuse language embeddings. 
          </p>
        </figure>
      </div>
    </div>

    <h3 class="title is-4">Inference Time</h3>
    <div class="figure-card">
      <img src="static/images/12.png" alt="Inference time comparison" loading="lazy">
      <p class="figure-caption">
        LaRA-VLA significantly reduces inference latency, achieving 135 ms per rollout and outperforming all baselines by a large margin.
        Compared to explicit CoT methods, this yields up to a 90% reduction in inference time, demonstrating the efficiency benefits of latent
        reasoning without explicit CoT decoding.
      </p>
    </div>

    <h3 class="title is-4">Ablation</h3>
    <div class="figure-card ablation-card">
      <img src="static/images/10.png" alt="Ablation study" loading="lazy">
      <p class="figure-caption">
        Ablation study of different forms of CoT supervision on SimplerEnv. <b>TextCoT</b> denotes explicit textual chain of thought,
        <b>Latent TextCoT</b> denotes latent textual chain of thought, and <b>Latent VisCoT</b> denotes latent visual chain of thought.
      </p>
    </div>
  </div>
</section>
<!-- End Analysis -->
  </main>

  </body>
  </html>
