<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models - Jing Lyu">
  <meta name="description" content="LaRA-VLA internalizes chain-of-thought reasoning into continuous latent representations for efficient vision-language-action control, reducing inference latency while improving performance on simulation and real-robot tasks.">
  <meta name="keywords" content="vision-language-action, VLA, chain-of-thought, latent reasoning, embodied AI, robotics, manipulation, imitation learning">
  <meta name="author" content="Jing Lyu">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Latent-Reasoning-VLA">
  <meta property="og:title" content="Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models">
  <meta property="og:description" content="LaRA-VLA internalizes chain-of-thought reasoning into continuous latent representations for efficient vision-language-action control, reducing inference latency while improving performance on simulation and real-robot tasks.">
  <!-- NOTE: Using relative paths for local preview/static hosting. For best social sharing, use absolute URLs. -->
  <meta property="og:url" content="./">
  <meta property="og:image" content="static/images/1.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Latent Reasoning VLA - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Jing Lyu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="vision-language-action">
  <meta property="article:tag" content="latent reasoning">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Latent Reasoning VLA (LaRA-VLA)">
  <meta name="twitter:description" content="Latent chain-of-thought reasoning for efficient vision-language-action control. Up to 90% lower inference latency vs explicit CoT while improving performance on simulation and real-robot tasks.">
  <meta name="twitter:image" content="static/images/1.png">
  <meta name="twitter:image:alt" content="Latent Reasoning VLA - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models">
  <meta name="citation_author" content="Lyu, Jing">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="ICML 2026">
  <meta name="citation_pdf_url" content="static/pdfs/ICML_2026_Latent_Reasoning_VLA.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Latent Reasoning VLA (LaRA-VLA) | Project Page</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models",
    "description": "LaRA-VLA internalizes chain-of-thought reasoning into continuous latent representations for efficient vision-language-action control.",
    "author": [
      {
        "@type": "Person",
        "name": "Jing Lyu",
        "affiliation": {
          "@type": "Organization",
          "name": "Institution Name"
        }
      }
    ],
    "datePublished": "2026-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "Under review"
    },
    "url": "./",
    "image": "static/images/1.png",
    "keywords": ["vision-language-action", "VLA", "chain-of-thought", "latent reasoning", "embodied AI", "robotics"],
    "abstract": "Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (LaRA-VLA), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control.",
    "citation": "@misc{lyu2026lara_vla,\\n  title={Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models},\\n  author={Lyu, Jing},\\n  year={2026},\\n  note={Under review}\\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "./"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Vision-Language-Action"
      },
      {
        "@type": "Thing",
        "name": "Embodied AI"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Institution Name",
    "url": "./",
    "logo": "static/images/favicon.ico"
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown (restore this block if you want to showcase more works) -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank" rel="noopener noreferrer">Jing Lyu</a>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="static/pdfs/ICML_2026_Latent_Reasoning_VLA.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="#" class="external-link button is-normal is-rounded is-dark" aria-disabled="true"
                        title="Code link coming soon" onclick="return false;">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code (Coming soon)</span>
                    </a>
                  </span>

                    <span class="link-block">
                      <a href="static/videos/lara-vla_real_world_tasks.mp4" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-video"></i>
                      </span>
                      <span>Real-World Tasks (MP4)</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <source src="static/videos/lara-vla_real_world_tasks.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Real-world long-horizon manipulation tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (LaRA-VLA), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="static/images/1.png" alt="Figure 1" loading="lazy"/>
        </div>
        <div class="item">
          <img src="static/images/2.png" alt="Figure 2" loading="lazy"/>
        </div>
        <div class="item">
          <img src="static/images/3.png" alt="Figure 3" loading="lazy"/>
        </div>
        <div class="item">
          <img src="static/images/5.png" alt="Figure 5" loading="lazy"/>
        </div>
        <div class="item">
          <img src="static/images/6.png" alt="Figure 6" loading="lazy"/>
        </div>
        <div class="item">
          <img src="static/images/7.png" alt="Figure 7" loading="lazy"/>
        </div>
        <div class="item">
          <img src="static/images/8.png" alt="Figure 8" loading="lazy"/>
        </div>
        <div class="item">
          <img src="static/images/9.png" alt="Figure 9" loading="lazy"/>
        </div>
      </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- (Optional) Video Presentation / Additional Videos -->
<!-- If you have a YouTube talk or more qualitative videos, add them here. -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <!-- TODO: Replace with your poster PDF -->
      <iframe  src="static/pdfs/ICML_2026_Latent_Reasoning_VLA.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{lyu2026lara_vla,
  title={Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models},
  author={Lyu, Jing},
  year={2026},
  note={Under review}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
